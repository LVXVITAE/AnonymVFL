from SharedVariable import SharedVariable
import numpy as np
from tqdm import trange, tqdm
from common import out_dom
import jax.numpy as jnp
import secretflow as sf
from secretflow.device import SPUObject, PYUObject
from secretflow import SPU, PYU
from secretflow.data import FedNdarray, PartitionWay
from secretflow.data.ndarray import load
from common import approx_sigmoid, sigmoid, softmax, load_dataset
import os, json

class SSLR:
    def __init__(self, devices: dict, lambda_ : float = 0, approx : bool = True):
        """
        ## Args: 
         - devices : åº”åŒ…å«å››ä¸ªå­—æ®µï¼Œæ¯ä¸ªå­—æ®µçš„å€¼åº”ä¸ºSPUæˆ–PYUã€‚ä¾‹å¦‚ï¼š

           devices = {
            'spu': spu,
            'company': company,
            'partner': partner,
           }

         - lambda_: l2æ­£åˆ™åŒ–å‚æ•°ï¼Œé»˜è®¤ä¸º0ã€‚
         - approx: æ˜¯å¦ä½¿ç”¨è¿‘ä¼¼sigmoidå‡½æ•°ï¼Œé»˜è®¤ä¸ºTrueã€‚
         è¿™é‡Œæä¾›äº†LRçš„ä¸¤ç§å®ç°ã€‚å¦‚æœapproxä¸ºTrueï¼Œåˆ™ä½¿ç”¨çº¿æ€§åˆ†æ®µå‡½æ•°è¿‘ä¼¼sigmoidå‡½æ•°ã€‚å¤šåˆ†ç±»åœºæ™¯ä¸‹æœ¬æ¨¡å—ä¼šè®­ç»ƒå¤šä¸ªå¹³è¡Œçš„2åˆ†ç±»å™¨ï¼ˆç„¶è€Œè¿™æ ·ä¼šå¯¼è‡´æ¯ä¸ªåˆ†ç±»å™¨è¾“å‡ºä¹‹å’Œä¸ä¸º1ï¼Œåç»­å¯è€ƒè™‘æ˜¯å¦æœ‰æ›´å¥½çš„å¤šåˆ†ç±»ç®—æ³•ï¼‰ã€‚
         å¦‚æœapproxä¸ºFalseï¼Œåˆ™ä¸è¿‘ä¼¼sigmoidå‡½æ•°ã€‚åŒæ–¹å…ˆç”¨å®‰å…¨å¤šæ–¹ä¹˜æ³•è®¡ç®—z = X @ wçš„ç»“æœï¼Œå†å°†zå‘é€åˆ°yçš„æŒæœ‰è€…ï¼ˆyä¸ä½œç§˜å¯†å…±äº«ï¼‰ï¼Œç”±yçš„æŒæœ‰è€…è®¡ç®—sigmoidå’Œæ¢¯åº¦ã€‚å¤šåˆ†ç±»åœºæ™¯ä¸‹ç”¨æœ¬æ¨¡å—softmaxå‡½æ•°ä»£æ›¿sigmoidå‡½æ•°ã€‚
        """
        self.lambda_ = lambda_
        self.approx = approx
        assert 'spu' in devices and isinstance(devices['spu'], SPU), "devices must contain 'spu' of type SPU"
        self.spu = devices['spu']
        self.company = devices['company']
        self.partner = devices['partner']

    def _forward(self, X : SPUObject) -> SPUObject | PYUObject:
        """
        ## Args:
         - X: è¾“å…¥ç§˜å¯†å…±äº«çš„ç‰¹å¾çŸ©é˜µ
        """
        def matmul(X, w):
            return X @ w
        z = self.spu(matmul)(X, self.w)
        z = z.to(self.train_label_keeper) # å°†zå‘é€ç»™æ ‡ç­¾yçš„æŒæœ‰
        if self.approx:
            activate_fn = approx_sigmoid
        else:
            if self.out_features == 1:
                activate_fn = sigmoid
            else:
                activate_fn = softmax

        return self.train_label_keeper(activate_fn)(z)

    def dispatch_weight(self):
        assert isinstance(self.w, SPUObject), "Weights must be on SPU"
        def get_item(arr : jnp.ndarray, keys):
            return arr[keys]
        w1, w2= self.spu(get_item, static_argnames=['keys'])(self.w, np.arange(self.split_col)), self.spu(get_item, static_argnames=['keys'])(self.w, np.arange(self.split_col, self.in_features))
        w1 = w1.to(self.company)
        w2 = w2.to(self.partner)
        w = load({self.company: w1, self.partner: w2}, partition_way=PartitionWay.HORIZONTAL)
        return w

    def predict(self, X : FedNdarray, device : PYU, threshold: float = 0.33)  -> PYUObject:
        """
        ## Args:
         - X: è¾“å…¥çºµå‘åˆ’åˆ†çš„ç‰¹å¾çŸ©é˜µ
         - device: é¢„æµ‹ç»“æœå­˜æ”¾çš„PYUè®¾å¤‡
         - threshold: åˆ†ç±»é˜ˆå€¼ï¼Œé»˜è®¤0.5ï¼Œå¯¹äºä¸å¹³è¡¡æ•°æ®å¯ä»¥è°ƒä½
        """
        assert isinstance(X, FedNdarray), "X must be a FedNdarray"
        assert isinstance(device,PYU), 'predictions must be moved to a PYU device'

        if isinstance(self.w, SPUObject):
            w = self.dispatch_weight()
        elif isinstance(self.w, FedNdarray):
            w = self.w
        z1 = self.company(lambda X, w: X @ w)(X.partitions[self.company], w.partitions[self.company]).to(device)
        z2 = self.partner(lambda X, w: X @ w)(X.partitions[self.partner], w.partitions[self.partner]).to(device)
        
        if self.approx:
            activate_fn = approx_sigmoid
        else:
            if self.out_features == 1:
                activate_fn = sigmoid
            else:
                activate_fn = softmax
            
        y = device(lambda a, b: activate_fn(a + b))(z1, z2)

        def to_int_labels(logits : np.ndarray, threshold):
            #å°†logitè½¬åŒ–ä¸ºæ•´æ•°æ ‡ç­¾ï¼Œä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼
            if logits.shape[1] == 1:
                return (logits > threshold).astype(int)
            else:
                return np.argmax(logits, axis=1)
        y = device(to_int_labels, static_argnames=['threshold'])(y, threshold)

        return y

    def _backward(self, X : SPUObject, y : SPUObject | PYUObject, y_pred : SPUObject | PYUObject, lr : float = 0.1):
        """
        æ¢¯åº¦ä¸‹é™æ­¥éª¤
        ## Args:
         - X: è¾“å…¥ç§˜å¯†å…±äº«çš„ç‰¹å¾çŸ©é˜µ
         - y: æ ‡ç­¾ï¼Œç§˜å¯†å…±äº«æˆ–æ˜æ–‡
         - y_pred: æ¨¡å‹é¢„æµ‹ç»“æœï¼Œç§˜å¯†å…±äº«æˆ–æ˜æ–‡
         - lr: æ¢¯åº¦ä¸‹é™æ­¥é•¿ï¼Œé»˜è®¤ä¸º0.1
        """
        assert y.device == y_pred.device, "y and y_pred must be on the same device"
        def compute_gradient(y_pred, y):
            return y_pred - y
        grad = self.train_label_keeper(compute_gradient)(y_pred, y)
        grad = grad.to(self.spu)
        def grad_desc(lambda_, w : jnp.ndarray, X : jnp.ndarray, grad : jnp.ndarray):
            batch_size = X.shape[0]
            return (1 - lambda_) * w - (lr/batch_size) * (X.transpose() @ grad)
        self.w = self.spu(grad_desc)(self.lambda_, self.w, X, grad)

    def fit(self, X : SPUObject, y : SPUObject | PYUObject, X_test : FedNdarray | None = None, y_test : PYUObject | None = None, batch_size = 64, val_steps = 1, n_epochs = 10, lr = 0.1, split_col : int = None):
        """
        è®­ç»ƒæŒ‡å®šè½®æ•°
        ## Args:
        - X: è®­ç»ƒé›†ç‰¹å¾çŸ©é˜µã€‚
        - y: è®­ç»ƒé›†æ ‡ç­¾ã€‚
        - X_test: éªŒè¯é›†ç‰¹å¾çŸ©é˜µã€‚å¦‚æä¾›ï¼Œå°†ç›¸éš”è‹¥å¹²stepåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°å‡†ç¡®ç‡ã€‚
        - y_test: éªŒè¯é›†æ ‡ç­¾ã€‚
        - batch_size: æ¯ä¸ªbatchçš„æ ·æœ¬æ•°é‡ï¼Œé»˜è®¤ä¸º64ã€‚
        - val_steps: æ¯éš”å¤šå°‘ä¸ªstepåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ä¸€æ¬¡ã€‚æ¯æ›´æ–°ä¸€æ¬¡æƒé‡ç®—ä¸€ä¸ªstepã€‚é»˜è®¤ä¸º1ã€‚
        - n_epochs: è®­ç»ƒè½®æ•°ï¼Œé»˜è®¤ä¸º10ã€‚
        - lr: åˆå§‹å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º0.1ã€‚å­¦ä¹ ç‡ä¼šéšç€è¿­ä»£æ¬¡æ•°æˆåæ¯”ã€‚
        - split_col: åˆ’åˆ†companyç‰¹å¾å’Œpartnerç‰¹å¾çš„åˆ—ã€‚å·¦ä¾§æ˜¯companyçš„ç‰¹å¾ï¼Œå³ä¾§æ˜¯partnerçš„ç‰¹å¾ã€‚å¦‚æœªæä¾›éªŒè¯é›†åˆ™å¿…é¡»æä¾›æ­¤é¡¹ã€‚
        ## Returns:
        - accs: å¦‚æœæä¾›äº†éªŒè¯é›†ï¼Œåˆ™è¿”å›æ¯æ¬¡åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°çš„å‡†ç¡®ç‡ã€‚

        æ³¨æ„ï¼šè®­ç»ƒå®Œæˆåï¼Œself.wä»¥æ˜æ–‡çš„å½¢å¼å­˜å‚¨ï¼Œcompanyå’Œpartnerå„è‡ªæŒæœ‰ä¸€éƒ¨åˆ†ã€‚æ¨ç†æ—¶åŒæ–¹åˆ†åˆ«å°†å„è‡ªçš„ç‰¹å¾ä¸å„è‡ªçš„wç›¸ä¹˜ï¼Œç„¶åç”±æ ‡ç­¾æŒæœ‰è€…èšåˆç»“æœã€‚
        """
        assert isinstance(X, SPUObject) and X.device == self.spu, "X must be on SPU"
        assert (isinstance(y, SPUObject) and y.device == self.spu) or (isinstance(y, PYUObject)), "y must be on active_party PYU or SPU"
        self.train_label_keeper = y.device
        num_samples, self.in_features = sf.reveal(self.spu(np.shape)(X))
        _, self.out_features = sf.reveal(self.train_label_keeper(np.shape)(y))
        self.in_features = int(self.in_features)
        self.out_features = int(self.out_features)
        self.w = np.zeros((self.in_features, self.out_features),dtype=np.float32)

        assert X_test is not None or split_col is not None, "Either validate set or split col must be provided"
        self.split_col = split_col if split_col is not None else sf.reveal(X_test.partition_shape()[self.company])[1]

        Xs = []
        ys = []
        validate = X_test is not None and y_test is not None
        if validate:
            assert isinstance(X_test, FedNdarray), "X_test must be a FedNdarray"
        if not self.approx:
            assert y.device != self.spu, "When approx is False, y must not be on SPU"
        for j in trange(0,num_samples,batch_size):
            batch = min(batch_size,num_samples - j)
            keys = np.arange(j, j + batch)
            def get_item(arr : jnp.ndarray, keys):
                return arr[keys]
            X_batch = self.spu(get_item, static_argnames=['keys'])(X, keys)
            y_batch = self.train_label_keeper(get_item, static_argnames=['keys'])(y, keys)
            Xs.append(X_batch)
            ys.append(y_batch)
        steps = 0
        accs = []
        f1s = []
        fOrs = []
        finalacc = 0
        finalf1 = 0
        finalfOr = 0
    
        for t in range(1,n_epochs + 1):
            print(f"Epoch {t}")
            for X,y in tqdm((zip(Xs, ys))):
                y_pred = self._forward(X)
                # å­¦ä¹ ç‡éšç€è¿­ä»£æ¬¡æ•°é€’å‡
                self._backward(X, y, y_pred, lr / t)
                if validate and steps % val_steps == 0:
                    y_pred = self.predict(X_test, y_test.device)
                    def compute_accuracy(y_true : np.ndarray, y_pred : np.ndarray):
                        y_true = y_true.reshape(-1,1)
                        y_pred = y_pred.reshape(-1,1)
                        # è°ƒè¯•è¾“å‡ºï¼šæŸ¥çœ‹y_trueå’Œy_predçš„å®é™…å€¼
                        # print(f"DEBUG - y_true unique values: {np.unique(y_true)}")
                        # print(f"DEBUG - y_pred unique values: {np.unique(y_pred)}")
                        # print(f"DEBUG - y_true shape: {y_true.shape}, y_pred shape: {y_pred.shape}")
                        # print(f"DEBUG - y_true first 100 values: {y_true[:100].flatten()}")
                        # print(f"DEBUG - y_pred first 100 values: {y_pred[:100].flatten()}")
                        return np.mean(y_true == y_pred)
                    def compute_f1(y_true : np.ndarray, y_pred : np.ndarray):
                        y_true = y_true.reshape(-1,1)
                        y_pred = y_pred.reshape(-1,1)
                        # print(f"DEBUG F1 - y_true range: [{np.min(y_true)}, {np.max(y_true)}]")
                        # print(f"DEBUG F1 - y_pred range: [{np.min(y_pred)}, {np.max(y_pred)}]")
                        # ä½¿ç”¨æ›´å®‰å…¨çš„æ¯”è¾ƒæ–¹å¼ï¼Œå¤„ç†æµ®ç‚¹æ•°
                        tp = np.sum((np.abs(y_true - 0.0) < 1e-6) & (np.abs(y_pred - 0.0) < 1e-6))
                        fp = np.sum((np.abs(y_true - 1.0) < 1e-6) & (np.abs(y_pred - 0.0) < 1e-6))
                        fn = np.sum((np.abs(y_true - 0.0) < 1e-6) & (np.abs(y_pred - 1.0) < 1e-6))
                        # print(f"DEBUG F1 - TP: {tp}, FP: {fp}, FN: {fn}")
                        # F1åˆ†æ•°
                        precision = tp / (tp + fp + 1e-8)  # æ·»åŠ å°æ•°é¿å…é™¤é›¶
                        recall = tp / (tp + fn + 1e-8)
                        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)  
                        return f1
                    
                    def compute_fOr(y_true : np.ndarray, y_pred : np.ndarray):
                        y_true = y_true.reshape(-1,1)
                        y_pred = y_pred.reshape(-1,1)
                        # print(f"DEBUG FOR - y_true range: [{np.min(y_true)}, {np.max(y_true)}]")
                        # print(f"DEBUG FOR - y_pred range: [{np.min(y_pred)}, {np.max(y_pred)}]")
                        tp = np.sum((y_true == 0) & (y_pred == 0))
                        fp = np.sum((y_true == 1) & (y_pred == 0))
                        fn = np.sum((y_true == 0) & (y_pred == 1))
                        tn = np.sum((y_true == 1) & (y_pred == 1))
                        # print(f"DEBUG FOR - TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}")
                        # è¯¯æ¼ç‡ï¼ˆFalse omission rateï¼‰æŒ‡æ¨¡å‹é¢„æµ‹çš„å…¨éƒ¨é˜´æ€§ä¾‹æ•°ä¸­å®é™…æ‚£ç—…è€…æ‰€å æ¯”ä¾‹ï¼Œåæ˜ äº†æ¨¡å‹å‘ç°é˜´æ€§è€…ä¸­æ‚£ç—…çš„æƒ…å†µã€‚
                        fOr = fn / (fn + tn + 1e-8)
                        return fOr
                    
                    acc = y_test.device(compute_accuracy)(y_test, y_pred)
                    f1 = y_test.device(compute_f1)(y_test, y_pred)
                    fOr = y_test.device(compute_fOr)(y_test, y_pred)
                    acc = sf.reveal(acc)
                    f1 = sf.reveal(f1)
                    fOr = sf.reveal(fOr)
                    accs.append(acc)
                    f1s.append(f1)
                    fOrs.append(fOr)
                    if t == n_epochs:
                        if acc > finalacc:
                            finalacc = acc
                        if f1 > finalf1:
                            finalf1 = f1
                        if fOr < finalfOr:
                            finalfOr = fOr
                    print(f"Step {steps}, Accuracy: {acc:.4f}, F1: {f1:.4f}, FOR: {fOr:.4f}")
                steps += 1

        self.w = self.dispatch_weight()
        print(f"\nğŸ“ˆ æœ€ç»ˆéªŒè¯ç»“æœ:")
        print(f"   â€¢ æœ€ç»ˆå‡†ç¡®ç‡: {finalacc:.4f}")
        print(f"   â€¢ æœ€ç»ˆF1åˆ†æ•°: {finalf1:.4f}")
        print(f"   â€¢ æœ€ç»ˆè¯¯æ¼ç‡: {finalfOr:.4f}")
        return accs


    def save(self, paths : dict[str, str], ext = 'npy'):
        '''
        ## Args
        - paths: ä¿å­˜æ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„åˆ—è¡¨ï¼ŒåŒ…å«companyå’Œpartnerçš„è·¯å¾„ã€‚ä¾‹å¦‚ï¼š
        paths = {
            'company': 'path/to/company/model',
            'partner': 'path/to/partner/model'
        }
        '''
        assert isinstance(self.w, FedNdarray), "Weights must be a FedNdarray"
        w1, w2 = self.w.partitions[self.company], self.w.partitions[self.partner]
        info = {
            'shape' : (self.in_features, self.out_features),
            'lambda_': float(self.lambda_),
            'approx': bool(self.approx),
            'save_as' : ext
        }
        def save_model(w : np.ndarray, path : str):
            try:
                os.makedirs(path, exist_ok=True)
                print(f"Directory '{path}' created or already exists.")
            except OSError as e:
                print(f"Error creating directory '{path}': {e}")
            
            if ext == 'npy':
                np.save(os.path.join(path, 'weight.npy'), w)
            elif ext == 'csv':
                np.savetxt(os.path.join(path, 'weight.csv'), w, delimiter=',')
            json.dump(info, open(os.path.join(path, 'info.json'), 'w'))
        self.company(save_model)(w1, paths['company'])
        self.partner(save_model)(w2, paths['partner'])

    def load(self, paths):
        def load_model(path : str):
            info = json.load(open(os.path.join(path, 'info.json'), 'r'))
            ext = info['save_as']
            if ext == 'csv':
                w = np.loadtxt(os.path.join(path, 'weight.csv'), delimiter=',')
            else:
                w = np.load(os.path.join(path, 'weight.npy'))
            return w, info
        w1, info1 = self.company(load_model)(paths['company'])
        w2, info2 = self.partner(load_model)(paths['partner'])
        assert info1 == info2, "Model info mismatch between company and partner"
        self.w = load({self.company: w1, self.partner: w2}, partition_way=PartitionWay.HORIZONTAL)
        self.in_features, self.out_features = info1['shape']
        self.lambda_ = info1['lambda_']
        self.approx = info1['approx']

# è¿è¡Œæœ¬æ–‡ä»¶ç›´æ¥æ‰§è¡Œè¿™ä¸ªå‡½æ•°
def SSLR_test(dataset):
    """
    ï¼ˆä¸æ‰§è¡ŒPSIï¼‰æµ‹è¯•SSLRæ€§èƒ½ï¼Œç»˜åˆ¶æŸå¤±æ›²çº¿
    """
    from common import MPCInitializer
    mpc_init = MPCInitializer()
    spu = mpc_init.spu
    company = mpc_init.company
    partner = mpc_init.partner
    coordinator = mpc_init.coordinator
    devices = {
        'spu': spu,
        'company': company,
        'partner': partner,
        'coordinator': coordinator,
        'active_party': company
    }

    train_X, train_y, test_X, test_y = load_dataset(dataset)
    split_col = train_X.shape[1] // 2
    num_cat = train_y.shape[1] if len(train_y.shape) > 1 else 1
    test_X = load({company : sf.to(company, test_X[:, :split_col]), partner : sf.to(partner, test_X[:, split_col:])})
    test_y = sf.to(company, test_y)
    train_X = sf.to(company, train_X).to(spu)
    train_y = sf.to(company, train_y).to(spu)

    model = SSLR(devices, approx=True)
    accs = model.fit(train_X, train_y, X_test=test_X, y_test=test_y, n_epochs=10, batch_size=1024, val_steps=10, lr=0.1)
    model.save({
        'company': './company_model',
        'partner': './partner_model'
    },ext='csv')
    plt.plot(accs,label = "SSLR",color = "blue")

    test_X = np.hstack([sf.reveal(test_X.partitions[company]), sf.reveal(test_X.partitions[partner])])
    test_y = sf.reveal(test_y)
    train_X = sf.reveal(train_X)
    train_y = sf.reveal(train_y)

    # å¯¹æ¯”sklearnçš„LRå®ç°
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(max_iter = 10,penalty=None)

    if num_cat > 1:
        train_y = train_y.argmax(axis=1)

    model.fit(train_X,train_y.ravel())
    y_pred = model.predict(test_X)
    Accracy = accuracy_score(test_y, y_pred)
    plt.axhline(Accracy, 0, len(accs), label="LR sklearn", color = "red",linestyle = "--")

    plt.xlabel("nIter")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.title(f"SSLR_{dataset}")
    plt.savefig(f"SSLR_{dataset}.png")
    plt.close()

# æ—©æœŸæµ‹è¯•ç”¨ï¼Œå¯å¿½ç•¥
class LR:
    def __init__(self, in_features, out_features = 1, lambda_ = 0, appx_sigmoid = False):
        self.out_features = out_features
        self.w = np.zeros((in_features,out_features))
        self.appx_sigmoid = appx_sigmoid
        self.lambda_ = lambda_

    def activate_fn(self,X : np.ndarray):
        if self.appx_sigmoid:
            X += 1/2
            return np.clip(X,0,1)
        else:
            X = np.clip(X,-500,500)
            if self.out_features ==1:
                return 1/(1 + np.exp(-X))
            return np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)

    def forward(self, X : np.ndarray):
        return self.activate_fn(X @ self.w)
    
    def predict(self, X):
        y = self.activate_fn(X @ self.w).round()
        if self.out_features == 1:
            return y
        else:
            return y.argmax(axis=1).reshape(-1,1)

    def backward(self, X : np.ndarray, y : np.ndarray, y_pred : np.ndarray, lr = 0.1):
        batch_size = X.shape[0]
        diff = y_pred - y
        self.w = (1 - self.lambda_) * self.w - (lr/batch_size) * (X.transpose() @ diff)

    def fit(self, X : np.ndarray, y : np.ndarray, n_iter = 100, batch_size = 64):
        num_samples, num_features = X.shape
        for t in range(1,n_iter + 1):
            for j in range(0,num_samples,batch_size):
                batch = min(batch_size,num_samples - j)
                X_batch = X[j:j+batch]
                y_batch = y[j:j+batch]

                y_pred = self.forward(X_batch)
                self.backward(X_batch, y_batch, y_pred, lr = 0.1 / t)

#æ—©æœŸæµ‹è¯•ä½¿ç”¨ï¼Œå¯æš‚æ—¶å¿½ç•¥
class LRSS:
    def __init__(self, in_features, out_features = 1, lambda_ = 0):
        self.out_features = out_features
        self.w = SharedVariable(np.zeros((in_features,out_features)),np.zeros((in_features, out_features)))
        self.lambda_ = lambda_

    @staticmethod
    def activate_fn(X : SharedVariable):
        GT_idx = np.argwhere(X > 1/2)
        LT_idx = np.argwhere(X < -1/2)
        X += 1/2
        for i, j in GT_idx:
            X[i, j] = 1
        for i, j in LT_idx:
            X[i, j] = 0
        return X

    def forward(self, X : SharedVariable):
        return self.activate_fn(X @ self.w)
    
    def predict(self, X : np.ndarray):
        y = np.clip((X @ self.w.reveal()) + 1/2, 0, 1).round()
        if self.out_features == 1:
            return y
        else:
            return y.argmax(axis=1).reshape(-1,1)
    
    def backward(self, X : SharedVariable, y : SharedVariable, y_pred : SharedVariable, lr = 0.1):
        batch_size = X.shape()[0]
        diff = y_pred - y
        self.w = (1 - self.lambda_) * self.w - (lr/batch_size) * (X.transpose() @ diff)

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
# æ—©æœŸæµ‹è¯•ç”¨ï¼Œå¯å¿½ç•¥
def train(train_X : SharedVariable, train_y : SharedVariable, test_X, test_y, n_iter = 100, batch_size = 64) -> SharedVariable:
    num_samples, num_features = train_X.shape()
    _, num_cat = train_y.shape()
    
    model = LRSS(num_features, num_cat)
    accs = []
    max_acc = 0
    for t in range(1,n_iter + 1):
        print(f"Epoch {t}")
        for j in trange(0,num_samples,batch_size):
            batch = min(batch_size,num_samples - j)
            X = train_X[j:j+batch]
            y = train_y[j:j+batch]

            y_pred = model.forward(X)
            model.backward(X, y, y_pred, lr = 0.1 / t)

            y_pred = model.predict(test_X)
            Accracy = accuracy_score(test_y, y_pred)
            if Accracy > max_acc:
                max_acc = Accracy
                print(f"Iteration {t}, Batch {j//batch_size + 1}, Accuracy: {Accracy:.4f}")
        accs.append(Accracy)

    plt.plot(accs,label = "LR_SS",color = "blue")
    plt.axhline(max_acc, 0, len(accs), label="Max LR_SS", color = "blue",linestyle = ":")

    train_X = train_X.reveal()
    train_y = train_y.reveal()

    model = LR(num_features, num_cat,appx_sigmoid=True)
    accs = []
    max_acc = 0

    for t in range(1,n_iter + 1):
        print(f"Epoch {t}")
        for j in trange(0,num_samples,batch_size):
            batch = min(batch_size,num_samples - j)
            X = train_X[j:j+batch]
            y = train_y[j:j+batch]

            y_pred = model.forward(X)
            model.backward(X, y, y_pred, lr = 0.1 / t)

            y_pred = model.predict(test_X)
            Accracy = accuracy_score(test_y, y_pred)
            if Accracy > max_acc:
                max_acc = Accracy
        accs.append(Accracy)
    
    plt.plot(accs,label = "LR_without_SS", color = "red",linestyle = "--")
    plt.axhline(max_acc, 0, len(accs), label="Max LR_without_SS", color = "red",linestyle = ":")

    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(max_iter = n_iter,penalty=None)

    if num_cat > 1:
        train_y = train_y.argmax(axis=1)

    model.fit(train_X,train_y.ravel())
    y_pred = model.predict(test_X)
    Accracy = accuracy_score(test_y, y_pred)
    plt.axhline(Accracy, 0, len(accs), label="LR sklearn", color = "green",linestyle = ":")

    plt.xlabel("nIter")
    plt.ylabel("Accuracy")
    plt.legend()

# æ—©æœŸæµ‹è¯•ç”¨ï¼Œå¯å¿½ç•¥
def LR_test(dataset):
    train_X, train_y, test_X, test_y = load_dataset(dataset)

    train_X = SharedVariable.from_secret(train_X, out_dom)
    train_y = SharedVariable.from_secret(train_y, out_dom)
    train(train_X, train_y, test_X,test_y)
    plt.title(f"LR_{dataset}")
    plt.savefig(f"LR_{dataset}.png")
    plt.close()

if __name__ == "__main__":
    # LR_test("mnist")
    # for dataset in ["pima","pcs","uis","gisette","arcene"]:
    #     LR_test(dataset)
    # SSLR_test("breast")
    SSLR_test("shop")